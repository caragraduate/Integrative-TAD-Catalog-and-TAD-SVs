{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "technical-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-syndication",
   "metadata": {},
   "source": [
    "## convert the IS boundary to TAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handmade-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "## auto chr\n",
    "IS_5kb = pd.read_csv('directory/inter30_5kb.robust_cutoff_boundaries_100kb.bed', sep='\\t', header = None)\n",
    "IS_5kbnew1 = IS_5kb[[0, 1, 2]]\n",
    "IS_5kbnew1.columns = ['chr','IS_start','IS_end']\n",
    "IS_5kbnew1['IS_start'] = IS_5kbnew1['IS_start']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eligible-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_size = pd.read_csv('directory/hg38.chrom.sizes_byme', sep='\\t', header = None)\n",
    "invalid = ['chrX', 'chrY', 'chrMT']\n",
    "chr_size_new = chr_size[~chr_size[0].str.contains('|'.join(invalid))].copy()\n",
    "chr_size_new[0] = chr_size_new[0].str.replace('chr','').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fundamental-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-39c40c390437>:2: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for k, v in IS_5kbnew1.groupby(['chr']):\n"
     ]
    }
   ],
   "source": [
    "os.chdir('directory/')\n",
    "for k, v in IS_5kbnew1.groupby(['chr']):\n",
    "    v.to_csv(f'IS_boundary_start_end_chr{k}.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "creative-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range (1,23):\n",
    "    filename = 'IS_boundary_start_end_chr{}.csv'.format(k)\n",
    "    boundary = pd.read_csv(filename, sep=',', header = 0)\n",
    "    boundary = boundary.drop_duplicates()\n",
    "    start = boundary[['chr', 'IS_end']]\n",
    "    start_1 = start.iloc[:-1]\n",
    "    start_1.reset_index(inplace = True, drop = True)\n",
    "    end =  boundary[['IS_start']]\n",
    "    end_1 = end.iloc[1:]\n",
    "    end_1.reset_index(inplace = True, drop = True)\n",
    "    result = pd.concat([start_1, end_1], axis=1)\n",
    "    TAD1_end = boundary.iloc[0]['IS_start'].astype(int)\n",
    "    df2 = pd.DataFrame({'chr': [k], 'IS_end' : ['0'], 'IS_start' : TAD1_end})\n",
    "    TADlast_start = boundary.iloc[-1]['IS_end'].astype(int)\n",
    "    TAD1ast_end = chr_size_new.loc[chr_size_new[0] == k, 1].item()\n",
    "    df3 = pd.DataFrame({'chr': [k], 'IS_end' : TADlast_start, 'IS_start' : TAD1ast_end})\n",
    "    dfnew = pd.concat([df2, result, df3], ignore_index = True, axis = 0)\n",
    "    dfnew.columns = ['chr','TAD_start','TAD_end']\n",
    "    out_file = 'IS_TAD_start_end_chr{}.bed'.format(k)\n",
    "    dfnew.to_csv(out_file, index = False, sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "horizontal-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "root, dirs, files = next(os.walk(os.getcwd()))\n",
    "with open('all_IS_TAD_start_end.bed', 'a') as outfile:\n",
    "    for infile in files:\n",
    "        if infile.startswith('IS_TAD_start'):\n",
    "            df = pd.read_csv(os.path.join(root, infile), sep='\\t', skiprows=[0], header = None)\n",
    "            df = df.drop_duplicates()\n",
    "            df.to_csv(outfile, index=False, sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "central-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_csv('all_IS_TAD_start_end.bed', sep='\\t', header = None)\n",
    "f1new = f1[[0,1,2]]\n",
    "f1new.columns = ['chr','TAD_start','TAD_end']\n",
    "f1new.sort_values(by=['chr', 'TAD_start'], inplace=True)\n",
    "f1new.to_csv('final_IS_TAD_start_end_5kb.bed', index=False, sep='\\t', header = True)\n",
    "f1new['length'] = f1new['TAD_end'] - f1new['TAD_start']\n",
    "f1new.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "floating-stomach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate minimums in Pandas without `zero`-values?\n",
    "f1new[f1new > 0].loc[:, 'length'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aware-nation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1new_nozero = f1new[(f1new['length'] > 0)]\n",
    "f1new_nozero['length'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "classified-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1new_nozero.to_csv('/data/home/tun53987/Hi-C/experiment_mergeall/mega/aligned/fanc_insulation/final_IS_TAD_start_end_5kb.bed', index=False, sep='\\t', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "naked-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for 5kb:\n",
    "# since min_TAD is 5000bp, which includes 1 bin\n",
    "# we set that exclude if there is any TAD includes >= 1 NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-voltage",
   "metadata": {},
   "source": [
    "### check if the IS_TAD includes NA insulation score regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "welcome-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS = pd.read_csv('directory/inter30_5kb.insulation_100kb.bed', sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "hydraulic-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISnew = IS[[0,1,2,4]]\n",
    "ISnew.columns = ['chr','IS_start','IS_end','IS']\n",
    "ISnew['chr'] = ISnew['chr'].str.replace('chr','')\n",
    "ISnew['chr'] = ISnew['chr'].astype(str)\n",
    "invalid = ['X', 'Y', 'MT']\n",
    "ISnew1 = ISnew[~ISnew['chr'].str.contains('|'.join(invalid))]\n",
    "ISnew1['chr'] = ISnew1['chr'].astype(int)\n",
    "ISnew1['IS'] = ISnew1['IS'].astype(str)\n",
    "IS_NA = ISnew1[ISnew1['IS'] == 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "visible-franchise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>IS_start</th>\n",
       "      <th>IS_end</th>\n",
       "      <th>IS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>10000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>15000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>15001</td>\n",
       "      <td>20000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20001</td>\n",
       "      <td>25000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575005</th>\n",
       "      <td>22</td>\n",
       "      <td>50795001</td>\n",
       "      <td>50800000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575006</th>\n",
       "      <td>22</td>\n",
       "      <td>50800001</td>\n",
       "      <td>50805000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575007</th>\n",
       "      <td>22</td>\n",
       "      <td>50805001</td>\n",
       "      <td>50810000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575008</th>\n",
       "      <td>22</td>\n",
       "      <td>50810001</td>\n",
       "      <td>50815000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575009</th>\n",
       "      <td>22</td>\n",
       "      <td>50815001</td>\n",
       "      <td>50818468</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35257 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        chr  IS_start    IS_end   IS\n",
       "0         1         1      5000  nan\n",
       "1         1      5001     10000  nan\n",
       "2         1     10001     15000  nan\n",
       "3         1     15001     20000  nan\n",
       "4         1     20001     25000  nan\n",
       "...     ...       ...       ...  ...\n",
       "575005   22  50795001  50800000  nan\n",
       "575006   22  50800001  50805000  nan\n",
       "575007   22  50805001  50810000  nan\n",
       "575008   22  50810001  50815000  nan\n",
       "575009   22  50815001  50818468  nan\n",
       "\n",
       "[35257 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IS_NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-hotel",
   "metadata": {},
   "source": [
    "### find if the IS_TAD includes at least 10% of the length of the TADs are NA IS region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "joined-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def all_chr_na(chr_i):   \n",
    "    filename = 'IS_TAD_start_end_chr{}.bed'.format(chr_i)\n",
    "    TAD = pd.read_csv(filename, sep='\\t', header = 0)\n",
    "    IS_nan = IS_NA[IS_NA.chr == chr_i] \n",
    "    overlap = pd.DataFrame(columns=['chr', 'TAD_start', 'TAD_end', 'IS'])\n",
    "    for start, end in zip(TAD.loc[:, 'TAD_start'], TAD.loc[:, 'TAD_end']):\n",
    "        temp_overlap = IS_nan[IS_nan.loc[:, 'IS_start'].between(start, end) | IS_nan.loc[:, 'IS_end'].between(start, end)]\n",
    "        if temp_overlap.shape[0] >= 1:\n",
    "            temp_overlap2 = temp_overlap.copy(deep = True)\n",
    "            temp_overlap2['chr'] = IS_nan.loc[:, 'chr']\n",
    "            temp_overlap2['TAD_start'] = start\n",
    "            temp_overlap2['TAD_end'] = end\n",
    "            temp_overlap2['IS'] = IS_nan.loc[:, 'IS']\n",
    "            temp_overlap2['num_NA'] = int(temp_overlap.shape[0])\n",
    "            temp_overlap2['num_NA'] = temp_overlap2['num_NA']\n",
    "            overlap = pd.concat([overlap, temp_overlap2], axis=0).astype(str)\n",
    "\n",
    "    ### overlap file\n",
    "    final = overlap[['chr', 'TAD_start', 'TAD_end', 'IS', 'num_NA']].drop_duplicates()\n",
    "    final['length'] = final['TAD_end'].astype(int) - final['TAD_start'].astype(int)\n",
    "    #final['include_NA'] = final['lenth'].astype(int)//5000\n",
    "    out_file = 'NA_TAD_IS_start_end_chr{}.bed'.format(chr_i)\n",
    "    final.to_csv(out_file, index = False, sep='\\t', header=True)\n",
    "    \n",
    "    final_larger = final[final['num_NA'].astype(float).astype(int)*5000 > (final['length']//10).astype(int)]\n",
    "    out_file_1 = 'NA_larger_incluna_TAD_IS_start_end_chr{}.bed'.format(chr_i)\n",
    "    final_larger.to_csv(out_file_1, index = False, sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "daily-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5489 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "all_data = Parallel(n_jobs=40)(delayed(all_chr_na)(chr_i) for chr_i in range(1,23))\n",
    "end = time.time()\n",
    "print('{:.4f} s'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-degree",
   "metadata": {},
   "source": [
    "### remove the NA_TAD from the all_TAD result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "committed-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def all_chr_nona(chr_i):   \n",
    "    filename = 'IS_TAD_start_end_chr{}.bed'.format(chr_i)\n",
    "    TAD = pd.read_csv(filename, sep='\\t', header = 0)\n",
    "    filename_1 = 'NA_larger_incluna_TAD_IS_start_end_chr{}.bed'.format(chr_i)\n",
    "    IS_nan = pd.read_csv(filename_1, sep='\\t', header = 0, usecols=range(3))\n",
    "    final = pd.concat([TAD, IS_nan]).drop_duplicates(keep=False)\n",
    "    out_file = 'noNA_TAD_IS_start_end_chr{}.bed'.format(chr_i)\n",
    "    final.to_csv(out_file, index = False, sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "warming-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0579 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_data = Parallel(n_jobs=40)(delayed(all_chr_nona)(chr_i) for chr_i in range(1,23))\n",
    "end = time.time()\n",
    "print('{:.4f} s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "filled-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "root, dirs, files = next(os.walk(os.getcwd()))\n",
    "with open('all_noNA_IS_TAD_start_end.bed', 'a') as outfile:\n",
    "    for infile in files:\n",
    "        if infile.startswith('noNA_TAD_IS'):\n",
    "            df = pd.read_csv(os.path.join(root, infile), sep='\\t', skiprows=[0], header = None)\n",
    "            df = df.drop_duplicates()\n",
    "            df.to_csv(outfile, index=False, sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abstract-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = pd.read_csv('all_noNA_IS_TAD_start_end.bed', sep='\\t', header = None)\n",
    "f2new = f2[[0,1,2]]\n",
    "f2new.columns = ['chr','TAD_start','TAD_end']\n",
    "f2new.sort_values(by=['chr', 'TAD_start'], inplace=True)\n",
    "f2new['length'] = f2new['TAD_end'] - f2new['TAD_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "brilliant-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "### exclude the TAD with zero length\n",
    "f2new_nozero = f2new[(f2new['length'] > 0)]\n",
    "f2new_nozero.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "normal-upset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7260000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2new_nozero['length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "specific-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f2new_nozero.to_csv('final_noNA_IS_TAD_start_end_5kb.bed', index=False, sep='\\t', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-garage",
   "metadata": {},
   "source": [
    "### find the TADs that converted from IS but not having maxima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "actual-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TAD_noNA = pd.read_csv('directory/chr_final_noNA_IS_TAD_start_end_5kb.bed', sep='\\t')\n",
    "IS_TAD_containMaxima = pd.read_csv('directory/noNA_IS_TAD_containsMaxima.bed', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "overall-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TAD_containMaxima.columns = ['#chrchr', 'TAD_start', 'TAD_end', 'length']\n",
    "no_maxima = pd.concat([IS_TAD_noNA, IS_TAD_containMaxima]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ranking-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete the duplicate overlap_IS_12878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "supposed-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = pd.read_csv('/home/tun53987/Hi-C/experiment_mergeall/mega/aligned/inter30_contact_domains_list_experiment/overlap_IS_arrow_new_maxima.bed', sep='\\t', header = None)\n",
    "overlap.sort_values(by=[0, 1], inplace=True)\n",
    "#overlap\n",
    "df = overlap.drop_duplicates(subset=[0,1,2], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "wanted-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlap.to_csv('/home/tun53987/Hi-C/experiment_mergeall/mega/aligned/inter30_contact_domains_list_experiment/overlap_IS_arrow_new_sorted.bed', index=False, sep='\\t', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-weapon",
   "metadata": {},
   "source": [
    "### find the TADs that arrowhead detected but IS did not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "editorial-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = pd.read_csv('/home/tun53987/Hi-C/experiment_mergeall/mega/aligned/inter30_contact_domains_list_experiment/overlap_IS_arrow_new_maxima.bed', sep='\\t', header = None)\n",
    "overlap.columns = [\"IS_chr\", \"IS_Start\", \"IS_End\", \"IS_TAD_length\", \"#chr\", \"TAD_start\", \"TAD_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "chief-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrieve the regions that both arrow and IS detected but arrow has subTADs within it (detected more than one TAD)\n",
    "overlap_subTADs = overlap[overlap.duplicated(['IS_chr', 'IS_Start', 'IS_End'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adequate-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove overlap_subTADs from overlap to get the overlap_no_subTADs\n",
    "overlap_no_subTADs = pd.concat([overlap, overlap_subTADs]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "referenced-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "## overlap_IS is using \"overlap\" but not \"overlap_no_subTADs\" to filter because we would like to exculde all \n",
    "#overlepped IS and for those having subTADs we are using the arrowhead TADs results from overlap_subTADs\n",
    "overlap_IS = overlap[['IS_chr', \"IS_Start\", 'IS_End']]\n",
    "df_1 = overlap_IS.drop_duplicates(subset=['IS_chr', \"IS_Start\", 'IS_End'], keep='last')\n",
    "df_1.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "everyday-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_IS = pd.read_csv('/home/tun53987/Hi-C/experiment_mergeall/mega/aligned/fanc_insulation_5kb/noNA_IS_TAD_containsMaxima.bed', sep='\\t', header = 0)\n",
    "ori_IS.columns = [\"IS_chr\", \"IS_Start\", \"IS_End\", \"IS_TAD_length\"]\n",
    "ori_IS = ori_IS[['IS_chr', \"IS_Start\", 'IS_End']]\n",
    "no_overlap_1 = pd.concat([df_1, ori_IS]).drop_duplicates(keep=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "resistant-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_arrow = overlap[['#chr', \"TAD_start\", 'TAD_end']]\n",
    "df = overlap_arrow.drop_duplicates(subset=['#chr', \"TAD_start\", 'TAD_end'], keep='last')\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "smoking-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_arrow = pd.read_csv('/home/tun53987/Hi-C/experiment_mergeall/mega/aligned/inter30_contact_domains_list_experiment/arrow_auto_5000_blocks.bedpe', sep='\\t', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "pleasant-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_overlap = pd.concat([df, ori_arrow]).drop_duplicates(keep=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-forty",
   "metadata": {},
   "source": [
    "### generate overlap_arrow and overlap_IS to merge in bedtools merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "commercial-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_arrow = overlap_no_subTADs[['#chr', \"TAD_start\", 'TAD_end']]\n",
    "overlap_arrow.columns = ['chr', 'TAD_start', 'TAD_end']\n",
    "overlap_IS = overlap_no_subTADs[['IS_chr', \"IS_Start\", 'IS_End']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "exotic-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_arrow_IS = pd.concat([overlap_IS, overlap_arrow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "educated-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_overlap_arrow_IS_startend = pd.DataFrame({'TAD_Start': np.minimum(overlap_arrow_IS['IS_Start'], overlap_arrow_IS['TAD_start']),\n",
    "                           'TAD_End': np.maximum(overlap_arrow_IS['IS_End'], overlap_arrow_IS['TAD_end'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "smaller-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_overlap_arrow_IS = pd.concat([overlap_arrow_IS['chr'], merge_overlap_arrow_IS_startend], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "egyptian-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_overlap_arrow_IS.sort_values(by=['chr', 'TAD_Start'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-glory",
   "metadata": {},
   "source": [
    "### add no overlap from IS, no overlap from arrow and overlap_subTADs to merged_concat_overlapped_IS_arrow_recip.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "english-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrowhead\n",
    "no_overlap.columns = ['chr', 'TAD_Start', 'TAD_End']\n",
    "# IS\n",
    "no_overlap_1.columns = ['chr', 'TAD_Start', 'TAD_End']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "civil-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_subTADs_df = overlap_subTADs[['#chr', \"TAD_start\", 'TAD_end']]\n",
    "overlap_subTADs_df.columns = ['chr', 'TAD_Start', 'TAD_End']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "adequate-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to merge all\n",
    "all_TAD = pd.concat([no_overlap_1, no_overlap, overlap_subTADs_df, merge_overlap_arrow_IS])\n",
    "all_TAD.sort_values(by=['chr', 'TAD_Start'], inplace=True)\n",
    "all_TAD_nodup = all_TAD.drop_duplicates()\n",
    "all_TAD_nodup.sort_values(by=['chr', 'TAD_Start', 'TAD_End'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "trying-switzerland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18972, 3)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_TAD_nodup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fiscal-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### similar steps to generate the stringent TAD locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
